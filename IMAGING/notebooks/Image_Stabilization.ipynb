{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python notebook for pre-processing an ROI image stack.\n",
    "# Image Stabilization\n",
    "Assumes folder directory structure:\n",
    "<pre><code>  IMAGING\n",
    "    image_stacks\n",
    "    notebooks\n",
    "    results\n",
    "</code></pre>\n",
    "Execute the code sequentially, one block at a time, using &lt;shift-return&gt;.\n",
    "#### Initialize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import glob\n",
    "from ipyfilechooser import FileChooser\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.interpolate import splprep, splev\n",
    "from scipy.spatial import cKDTree\n",
    "from skimage import color, data, exposure, filters, io\n",
    "from skimage.draw import disk, circle_perimeter\n",
    "from skimage.feature import canny\n",
    "from skimage.morphology import binary_erosion, binary_dilation\n",
    "from skimage.morphology import remove_small_objects\n",
    "from skimage.restoration import denoise_bilateral, denoise_wavelet\n",
    "from skimage.util import img_as_ubyte, img_as_int, img_as_float\n",
    "import skimage.transform as tf\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# global variables\n",
    "if os.name == \"nt\":\n",
    "    FILE_SEP = \"\\\\\"\n",
    "else:\n",
    "    FILE_SEP = \"/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select an image stack file and set options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# global variables\n",
    "image_stack = \"\"    # the selected image stack\n",
    "image_bits = 10     # bits per pixel (can be found in the oir meta data)\n",
    "high_magnification = True\n",
    "\n",
    "s = {'description_width':'200px'} # a default widget style\n",
    "\n",
    "# create image files widget\n",
    "image_files = sorted([f.split(FILE_SEP)[-1] for f in glob.glob(\"../image_stacks/*.tif\", recursive=False)], key=str.casefold)\n",
    "image_widget = widgets.Select(options=image_files, description='Image stack', \n",
    "                            disabled=False, layout=widgets.Layout(width='400px'))\n",
    "# create image bits widget\n",
    "image_bits_widget = widgets.BoundedIntText(value=image_bits, min=8, max=16, step=1,\n",
    "                    description='Image data bits', disabled=False, layout={'width':'270px'}, style=s)\n",
    "# create high magnification widget\n",
    "high_magnification_widget = widgets.Checkbox(value=high_magnification, description='High magnification image?',\n",
    "                 disabled=False, indent=True)\n",
    "\n",
    "def f(w1,w2,w3):\n",
    "  global image_stack, high_magnification\n",
    "  image_stack = image_widget.value\n",
    "  image_bits = image_bits_widget.value\n",
    "  high_magnification = high_magnification_widget.value\n",
    "display(widgets.interactive(f, w1=image_widget, w3=image_bits_widget, w2=high_magnification_widget))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get an image stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load picture\n",
    "images = io.imread(\"../image_stacks/\" + image_stack)\n",
    "images = np.float32(images/(2.0**image_bits))\n",
    "zdepth = images.shape[0]\n",
    "for i in images:\n",
    "  for l in range(i.shape[0] - 1): # moving average over every two lines\n",
    "    i[l] = (i[l] + i[l+1]) / 2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: Use this code block to interactively explore landmark nuclei detection parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(8,8))\n",
    "\n",
    "@interact(\n",
    "  gn=widgets.FloatSlider(description='image gain',min=1.0, max=5.0, step=0.1, value=1.0),\n",
    "  sr=widgets.IntRangeSlider(description='stack range',min=0, max=zdepth, step=1, value=[0,8]), \n",
    "  bs=widgets.FloatSlider(description='BILATERAL sigma',min=0.0, max=4.0, step=0.1, value=1.0), \n",
    "  cs=widgets.FloatSlider(description='CANNY sigma',min=1.0, max=4.0, step=0.1, value=1.8), \n",
    "  ct=widgets.IntRangeSlider(description='threshold',min=0, max=100, step=1, value=[9,22]),\n",
    "  hr=widgets.IntRangeSlider(description='HOUGH radii',min=3, max=25, step=1, value=[x*(2 if high_magnification else 1) for x in[5,8]]),\n",
    "  hd=widgets.IntSlider(description='distance',min=5, max=50, step=1, value=10),\n",
    "  hp=widgets.IntSlider(description='peaks',min=50, max=500, step=10, value=270),\n",
    "  ht=widgets.FloatSlider(description='threshold',min=0.0, max=1.0, step=0.01, value=0.12),\n",
    "  cr=widgets.FloatSlider(description='circle ratio',min=1.0, max=2.0, step=0.01, value=1.2))\n",
    "\n",
    "def f(gn, sr, bs, cs, ct, hr, hd, hp, ht, cr):\n",
    "  A = gn*np.mean(images[sr[0]:sr[1]], axis=0) # the static images\n",
    "  A0 = A / np.amax(A) # normalize\n",
    "  imageA = color.gray2rgb(img_as_ubyte(A0))\n",
    "\n",
    "  # identify nuclei (circles)   \n",
    "  #A = filters.gaussian(A0, sigma=gs) # noise filter\n",
    "  #A = denoise_wavelet(A0, wavelet_levels=7, multichannel=False, rescale_sigma=False)\n",
    "  A = denoise_bilateral(A0, sigma_spatial=bs)\n",
    "  edges = canny(img_as_ubyte(A), sigma=cs, low_threshold=ct[0], high_threshold=ct[1])\n",
    "  hough_radii = np.arange(hr[0], hr[1], 1) # the range of radii to use in search\n",
    "  hough_res = tf.hough_circle(edges, hough_radii) # look for circles\n",
    "  accums, cy, cx, radii = tf.hough_circle_peaks(hough_res, hough_radii, min_xdistance=hd, \n",
    "                                           min_ydistance=hd, total_num_peaks=hp, \n",
    "                                           threshold=ht, normalize=False)\n",
    "\n",
    "  # remove false positives (bright disks with dark perimeter)\n",
    "  pix = [] # as an empty list (for the remaining center pixels)\n",
    "  for center_x, center_y, radius in zip(cx, cy, radii):\n",
    "    c = disk((center_x, center_y), radius, shape=A0.shape) # central disk\n",
    "    cp = circle_perimeter(center_x, center_y, radius+1, shape=A0.shape) # perimeter ring\n",
    "    if (np.mean(imageA[cp]) / np.mean(imageA[c])) > cr:\n",
    "      pix.append((center_x, center_y)) # dark disks with bright perimeter are OK\n",
    "\n",
    "  # remove duplicates (close center pixels)\n",
    "  pix = np.array(pix) # as a numpy array\n",
    "  tree = cKDTree(pix) # for pairwise distance query\n",
    "  rows_to_fuse = list(tree.query_pairs(r=8.0))\n",
    "  p = np.ones(pix.shape[0])           # array of \"keep\" flags\n",
    "  if(len(rows_to_fuse)):\n",
    "    p[np.array(rows_to_fuse)[:,0]] = 0  # flag the first of all duplicate pairs for deletion\n",
    "  pixx = pix[p.astype(bool)]          # the remaining center pixels\n",
    "\n",
    "  # draw nuclei centre pixels\n",
    "  for i in pixx:\n",
    "    #imageA[i[0], i[1]] = (255,0,0)\n",
    "    imageA[disk((i[0], i[1]), 1.1, shape=A0.shape)] = (255,0,0)\n",
    "  \n",
    "  ax.cla()\n",
    "  ax.imshow(imageA, norm=None)\n",
    "  plt.show()\n",
    "  return(str(pixx.shape[0]) + \" nuclei identified\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find all landmark nuclei in the image stack.\n",
    "NOTE: Can take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# landmark detection paramters\n",
    "bs = 1.0 \n",
    "cs = 1.8\n",
    "ct = [9,22]\n",
    "hr = [x*(2 if high_magnification else 1) for x in[5,8]]\n",
    "hd = 10\n",
    "hp = 270\n",
    "ht = 0.12\n",
    "cr = 1.2\n",
    "\n",
    "pixx = [] # a list of all the landmark nuclei centers\n",
    "min_n = 100000   # the least number of nuclei identified in a frame\n",
    "max_n = 0        # the most number of nuclei identified in a frame\n",
    "\n",
    "print(\"Processing frame: \", end = '')\n",
    "for i in range(3,images.shape[0]-3): # use moving average over seven frames\n",
    "  A = np.mean(images[i-3:i+4], axis=0)\n",
    "  A0 = A / np.amax(A) # normalized\n",
    "\n",
    "  # identify nuclei (circles)   \n",
    "  #A = filters.gaussian(A0, sigma=gs) # noise filter\n",
    "  #A = denoise_wavelet(A0, wavelet_levels=7, multichannel=False, rescale_sigma=False)\n",
    "  A = denoise_bilateral(A0, sigma_spatial=bs)\n",
    "  edges = canny(img_as_ubyte(A), sigma=cs, low_threshold=ct[0], high_threshold=ct[1])\n",
    "  hough_radii = np.arange(hr[0], hr[1], 1) # the range of radii to use in search\n",
    "  hough_res = tf.hough_circle(edges, hough_radii) # look for circles\n",
    "  accums, cx, cy, radii = tf.hough_circle_peaks(hough_res, hough_radii, min_xdistance=hd, \n",
    "                                           min_ydistance=hd, total_num_peaks=hp, \n",
    "                                           threshold=ht, normalize=False)\n",
    "\n",
    "  # remove false positives (bright disks with dark perimeter)\n",
    "  pix = [] # as an empty list (for the remaining center pixels)\n",
    "  for center_y, center_x, radius in zip(cy, cx, radii):\n",
    "    c = disk((center_y, center_x), radius, shape=A0.shape) # central disk\n",
    "    cp = circle_perimeter(center_y, center_x, radius+1, shape=A0.shape) # perimeter ring\n",
    "    if (np.mean(A0[cp]) / np.mean(A0[c])) > cr:\n",
    "      pix.append((center_x, center_y)) # dark disks with bright perimeter are OK\n",
    "\n",
    "  # remove duplicates (close center pixels)\n",
    "  pix = np.array(pix) # as a numpy array\n",
    "  tree = cKDTree(pix) # for pairwise distance query\n",
    "  rows_to_fuse = list(tree.query_pairs(r=8.0))\n",
    "  p = np.ones(pix.shape[0])           # array of \"keep\" flags\n",
    "  if(len(rows_to_fuse)):\n",
    "    p[np.array(rows_to_fuse)[:,0]] = 0  # flag the first of all duplicate pairs for deletion\n",
    "\n",
    "  # get counts and append to the landmark list\n",
    "  temp = np.full((np.count_nonzero(p),1),np.float(i))\n",
    "  pp = pix[p.astype(bool)].astype(float)\n",
    "  pp = np.concatenate((pp,temp),axis=1)\n",
    "  pp = list(map(tuple,pp)) # the remaining center pixels \n",
    "  pixx += pp\n",
    "  c = np.count_nonzero(p)\n",
    "  if c < min_n:\n",
    "    min_n = c\n",
    "  if c > max_n:\n",
    "    max_n = c    \n",
    "  print(str(i) + \", \", end = '')\n",
    "print(\"DONE.\")\n",
    "print(\"Range of per frame nuclei identified:\", str(min_n) + '-' + str(max_n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: Plot all landmark nuclei centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# plot landmarks\n",
    "plt.close() # frees up memory\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"landmark nuclei centers\")\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "tp = np.array(pixx)\n",
    "ax.scatter(tp[:,0],tp[:,1],tp[:,2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Identify and plot landmark \"threads\" to use for image stabilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# identify and plot landmark threads\n",
    "\n",
    "plt.close() # frees up memory\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"landmark threads\")\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "# distance based spatial clustering\n",
    "tp = np.array(pixx)\n",
    "tpp = tp * [1.0,1.0,0.5] # compress the z scale\n",
    "db = DBSCAN(eps=10, min_samples=10).fit(tpp)\n",
    "labels = db.labels_\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "# get cluster and noise counts\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "print('Number of landmark threads: %d' % n_clusters_)\n",
    "print('Number of deleted noise points: %d' % n_noise_)\n",
    "\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "  class_member_mask = (labels == k)\n",
    "  xy = tp[class_member_mask & core_samples_mask]\n",
    "  ax.scatter(xy[:, 0], xy[:, 1], xy[:, 2], color=tuple(col))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: Identify and plot a sample thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# get the longest thread\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "thread = unique[np.where(counts==np.max(counts[1:]))][0]\n",
    "\n",
    "plt.close() # frees up memory\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"sample thread\")\n",
    "\n",
    "ax = Axes3D(fig)\n",
    "ax.set_xlim3d(0,images.shape[1])\n",
    "ax.set_ylim3d(0,images.shape[2])\n",
    "\n",
    "tpp = tp[labels==thread]\n",
    "ax.plot(tpp[:,0],tpp[:,1],tpp[:,2])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: Check thread smoothing parameters on the sample thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x = tpp[:,0]\n",
    "y = tpp[:,1]\n",
    "z = tpp[:,2]\n",
    "\n",
    "# smooth the thread\n",
    "tckp,u = splprep([x,y,z],k=3,nest=-1,s=4000)\n",
    "xnew,ynew,znew = splev(np.linspace(0,1,images.shape[0]),tckp)\n",
    "\n",
    "plt.close() # frees up memory\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"sample thread - smoothed\")\n",
    "\n",
    "ax = Axes3D(fig)\n",
    "ax.set_xlim3d(0,512)\n",
    "ax.set_ylim3d(0,512)\n",
    "\n",
    "ax.plot(xnew,ynew,znew)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stabilize the image stack using piece-wise affine transformation warping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# get a copy of the original stack\n",
    "A = img_as_float(io.imread(\"../image_stacks/\" + image_stack)) # convert to float\n",
    "out = np.copy(A)\n",
    "\n",
    "# find threads that span the stack\n",
    "# NOTE: there are no threads in the first or last three frames, so skip those\n",
    "tcount = 0     # spanning thread count\n",
    "ls = set(labels)\n",
    "ls.remove(-1)\n",
    "lxnew = []\n",
    "lynew = []\n",
    "for ll in ls:\n",
    "  f = (tp[labels==ll])[0,2]\n",
    "  l = (tp[labels==ll])[-1,2]\n",
    "  if f==3 and l==(A.shape[0]-4): # NOTE: there are no threads in the first or last three frames\n",
    "    tcount = tcount + 1\n",
    "    tpp = tp[labels==ll]\n",
    "    tckp,u = splprep([tpp[:,0],tpp[:,1],tpp[:,2]],s=4000,k=3,nest=-1)\n",
    "    xnew,ynew,znew = splev(np.linspace(0,1,out.shape[0]),tckp)\n",
    "    lxnew.append(xnew)\n",
    "    lynew.append(ynew)\n",
    "lxnew = np.array(lxnew)\n",
    "lynew = np.array(lynew)\n",
    "print(\"Found \" + str(tcount) + \" spanning threads.\")\n",
    "\n",
    "# find image cropping values (to eliminate black borders caused by translation)\n",
    "XL = np.max(-np.int(np.floor(np.min(lxnew-lxnew[:,0][:,None]))),0)\n",
    "XR = np.min(-np.int(np.ceil(np.max(lxnew-lxnew[:,0][:,None]))),0)\n",
    "YL = np.max(-np.int(np.floor(np.min(lynew-lynew[:,0][:,None]))),0)\n",
    "YR = np.min(-np.int(np.ceil(np.max(lynew-lynew[:,0][:,None]))),0)\n",
    "\n",
    "# translate the frame corners using the average of the spanning thread translations\n",
    "transx = np.mean(lxnew-lxnew[:,0][:,None], axis=0)\n",
    "transy = np.mean(lynew-lynew[:,0][:,None], axis=0)\n",
    "cornersx = np.full((lxnew.shape[1],4),[0,0,511,511]) + transx[:, None]\n",
    "cornersy = np.full((lynew.shape[1],4),[0,511,0,511]) + transy[:, None]\n",
    "lxnew = np.concatenate((lxnew, np.transpose(cornersx)))\n",
    "lynew = np.concatenate((lynew, np.transpose(cornersy)))\n",
    "lnew = np.array([lxnew, lynew])\n",
    "\n",
    "# piece-wise affine transformation warping\n",
    "print(\"Warping frame:\", end = '')\n",
    "for i in range(3, out.shape[0]-3):\n",
    "  print(' ' + str(i) + ',', end = '')\n",
    "  tform = tf.PiecewiseAffineTransform()\n",
    "  tform.estimate(np.transpose(lnew[:,:,i]), np.transpose(lnew[:,:,0]))\n",
    "  out[i] = tf.warp(A[i], tform.inverse)\n",
    "print(\" DONE.\")\n",
    "\n",
    "# save the stabilized image stack\n",
    "for i in range(3): # duplicate the first and last three frames\n",
    "  out[i] = out[3]\n",
    "  out[-(1+i)] = out[-4]\n",
    "io.imsave(\"../image_stacks/\" + image_stack[0:-4] + \"_stab.tif\", \n",
    "    img_as_int(out[:,YL:YR,XL:XR]), check_contrast=False)  # out[x,y] goes to image(y,x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: Saved a cropped copy of the original image stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# saved cropped copy of original image\n",
    "A = io.imread(\"../image_stacks/\" + image_stack)\n",
    "io.imsave(\"../image_stacks/\" + image_stack[0:-4] + \"_orig.tif\", \n",
    "    A[:,YL:YR,XL:XR], check_contrast=False)  # out[x,y] goes to image(y,x)\n",
    "print(\"DONE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
